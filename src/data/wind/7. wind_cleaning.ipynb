{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main script to clean wind data at the zip code, monthly level\n",
    "\n",
    "Modules: N/A <br>\n",
    "Author: Cornelia Ilin <br>\n",
    "Email: cilin@wisc.edu <br>\n",
    "Date created: May 14, 2021 <br>\n",
    "\n",
    "**Citations (data sources)**\n",
    "\n",
    "``Wind data:`` \n",
    "\n",
    "download the \"MERRA2_100.tavgM_2d_slv_Nx\" product; this provides monthly averages of U and V components\n",
    "\n",
    "1. https://search.earthdata.nasa.gov/search/granules?p=C1276812859-GES_DISC&pg[0][qt]=1991-01-01T00%3A00%3A00.000Z%2C2017-12-31T23%3A59%3A59.999Z&pg[0][gsk]=-start_date&q=MERRA-2%20tavgM&tl=1624239533!3!!&m=-0.0703125!0.0703125!2!1!0!0%2C2\n",
    "\n",
    "__Jordan steps for wind data__ \n",
    "  * Search for report \"M2TMNXSLV\"\n",
    "  * Narrow down scope with geoshape file for CA (NOT ZCTA)  \n",
    "      * Acquired here https://data.ca.gov/dataset/ca-geographic-boundaries \n",
    "        * **No VPN access seems to be permitted here**\n",
    "  * Query for 1991-01-01 00:00:00 - Today  \n",
    "      * Click recurring\n",
    "\n",
    "and data dictionary here:\n",
    "\n",
    "2. https://gmao.gsfc.nasa.gov/pubs/docs/Bosilovich785.pdf\n",
    "3. https://disc.gsfc.nasa.gov/datasets/M2T1NXSLV_5.12.4/summary\n",
    "\n",
    "\n",
    "``Shapefiles for California ZIP codes (2010 census):``\n",
    "\n",
    "4. https://www.census.gov/cgi-bin/geo/shapefiles/index.php?year=2010&layergroup=ZIP+Code+Tabulation+Areas\n",
    "\n",
    "__Jordan tweaks__\n",
    "  * There is an updated 2020 and 2022 ZCTA file available but I think it makes sense to keep the 2010 as that is what project started with and shouldn't have changed much\n",
    "\n",
    "``Installation errors with Geopandas:``\n",
    "\n",
    "5. https://stackoverflow.com/questions/54734667/error-installing-geopandas-a-gdal-api-version-must-be-specified-in-anaconda\n",
    "\n",
    "``How to compute wind speed and direction:``\n",
    "\n",
    "6. https://stackoverflow.com/questions/21484558/how-to-calculate-wind-direction-from-u-and-v-wind-components-in-r\n",
    "7. https://github.com/blaylockbk/Ute_WRF/blob/master/functions/wind_calcs.py\n",
    "\n",
    "``Wind speed and direction intuition:``\n",
    "\n",
    "8. http://colaweb.gmu.edu/dev/clim301/lectures/wind/wind-uv\n",
    "9. https://www.earthdatascience.org/courses/use-data-open-source-python/intro-vector-data-python/spatial-data-vector-shapefiles/intro-to-coordinate-reference-systems-python/\n",
    "\n",
    "``To create maps of this wind data:``\n",
    "\n",
    "and also used to provide intuition for winddir and windspeed\n",
    "\n",
    "10. https://disc.gsfc.nasa.gov/information/howto?title=How%20to%20calculate%20and%20plot%20wind%20speed%20using%20MERRA-2%20wind%20component%20data%20using%20Python\n",
    "\n",
    "\n",
    "``Error - MultiPolygon or multipoly is not iterable``  \n",
    "\n",
    "This seems to come from an error in version of shapely. Force install with a version below 2.0, I use `shapely==1.8.5` in my env.\n",
    "\n",
    "\n",
    "**Citations (persons)**\n",
    "1. N/A\n",
    "\n",
    "**Preferred environment**\n",
    "1. Code written in Jupyter Notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import netCDF4 as ncdf\n",
    "import os\n",
    "from datetime import date, timedelta\n",
    "from math import pi\n",
    "import fiona\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "import matplotlib.ticker as mticker\n",
    "\n",
    "# geography\n",
    "import geopandas as gpd\n",
    "import osmnx as ox\n",
    "import shapely\n",
    "from shapely.geometry import Point\n",
    "\n",
    "#Moved from sklearn.neighbors to sklearn.metrics following their package change\n",
    "import sklearn.metrics\n",
    "dist = sklearn.metrics.DistanceMetric.get_metric(\n",
    "    'haversine'\n",
    ")\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    'ignore'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define working directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in_dir_zip_shapes = 'C:/Users/cilin/Research/CA_hospitals/Input/raw_data/census_geo/shapefiles_zcta/'\n",
    "# in_dir = 'C:/Users/cilin/Research/CA_hospitals/Input/raw_data/winds/'\n",
    "# in_health = 'C:/Users/cilin/Research/CA_hospitals/Input/final_data/health/'\n",
    "# out_dir = 'C:/Users/cilin/Research/CA_hospitals/Input/final_data/winds/'\n",
    "\n",
    "#Local directories on my machine (not gdrive)\n",
    "in_dir_zip_shapes = 'wind/tl_2010_06_zcta510/'\n",
    "in_dir = 'wind/ca only/'\n",
    "in_health = 'health/'\n",
    "out_dir = 'wind/clean/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tree` not found.\n"
     ]
    }
   ],
   "source": [
    "%tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``read_clean wind``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_clean_wind(year):\n",
    "    \"\"\"\n",
    "    Read in wind data by year\n",
    "    \"\"\"\n",
    "    # create empty df\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    for file in os.listdir(in_dir):\n",
    "        if file.startswith('MERRA2') and file[-10:-6] == str(year):\n",
    "\n",
    "            ## read .nc file ##\n",
    "            ###################\n",
    "            data = ncdf.Dataset(\n",
    "                in_dir + file, mode='r'\n",
    "            )\n",
    "            # print metadata\n",
    "            #print(data)\n",
    "\n",
    "            # grab vars of interest ##\n",
    "            ##########################\n",
    "            # longitude and latitude\n",
    "            lons = data.variables['lon']\n",
    "            lats = data.variables['lat']\n",
    "            # 2-meter eastward wind m/s\n",
    "            U2M = data.variables['U2M']\n",
    "            # 2-meter northward wind m/s\n",
    "            V2M = data.variables['V2M']\n",
    "\n",
    "            # Replace vals #\n",
    "            ################\n",
    "            #\\_FillValues with NaNs:\n",
    "            U2M_nans = U2M[:]\n",
    "            V2M_nans = V2M[:]\n",
    "            _FillValueU2M = U2M._FillValue\n",
    "            _FillValueV2M = V2M._FillValue\n",
    "            U2M_nans[U2M_nans == _FillValueU2M] = np.nan\n",
    "            V2M_nans[V2M_nans == _FillValueV2M] = np.nan\n",
    "\n",
    "            # Add new vars #\n",
    "            ################\n",
    "            # calculate wind speed\n",
    "            wspd = np.sqrt(U2M_nans**2+V2M_nans**2)\n",
    "\n",
    "            # calculate wind direction in radians\n",
    "            wdir = np.arctan2(V2M_nans, U2M_nans)\n",
    "            #WDIR= (270-atan2(V,U)*180/pi)%360\n",
    "            \n",
    "            # transform wind direction from radians to degrees\n",
    "            #dir_to_degrees = np.mod(180+np.rad2deg(np.arctan2(V2M_nans, U2M_nans)), 360) # this computes \"wind is blowing from\"' meteorological convetion'\n",
    "            wdir_to_degrees = np.mod(np.rad2deg(wdir), 360) # this computes \"wind is blowing towards\" 'oceonographic convention', see here: https://www.esri.com/arcgis-blog/products/product/analytics/displaying-speed-and-direction-symbology-from-u-and-v-vectors/\n",
    "            \n",
    "            \n",
    "            ## transform to df ##\n",
    "            #####################\n",
    "            # create an empty df for wind speed and direction with size len(lats) x len(lons) \n",
    "            df_wdir = pd.DataFrame(index=lats[:], columns=lons[:])   \n",
    "            df_wspd = pd.DataFrame(index=lats[:], columns=lons[:])\n",
    "            \n",
    "            # create an empty df for u and v components with size len(lats) x len(lons) \n",
    "            df_u = pd.DataFrame(index=lats[:], columns=lons[:])\n",
    "            df_v = pd.DataFrame(index=lats[:], columns=lons[:])\n",
    "\n",
    "            # populate each row in the empty df above with the wdir_meteo and wspd data and u and v components\n",
    "            for idx, idx_val in enumerate(df_wdir.index):\n",
    "                df_wdir.loc[idx_val, :] = wdir_to_degrees[0][idx]\n",
    "                df_wspd.loc[idx_val, :] = wspd[0][idx]\n",
    "                df_u.loc[idx_val, :] = U2M_nans[0][idx]\n",
    "                df_v.loc[idx_val, :] = V2M_nans[0][idx]\n",
    "\n",
    "            # add index (latitude) as column\n",
    "            df_wdir.reset_index(\n",
    "                drop=False,\n",
    "                inplace=True\n",
    "            )\n",
    "            \n",
    "            df_wdir.rename(\n",
    "                columns={'index':'lat'},\n",
    "                inplace=True\n",
    "            )\n",
    "            \n",
    "            \n",
    "            df_wspd.reset_index(\n",
    "                drop=False,\n",
    "                inplace=True\n",
    "            )\n",
    "            \n",
    "            df_wspd.rename(\n",
    "                columns={'index':'lat'},\n",
    "                inplace=True\n",
    "            )\n",
    "            \n",
    "            df_u.reset_index(\n",
    "                drop=False,\n",
    "                inplace=True\n",
    "            )\n",
    "            \n",
    "            df_u.rename(\n",
    "                columns={'index':'lat'},\n",
    "                inplace=True\n",
    "            )\n",
    "            \n",
    "            df_v.reset_index(\n",
    "                drop=False,\n",
    "                inplace=True\n",
    "            )\n",
    "            \n",
    "            df_v.rename(\n",
    "                columns={'index':'lat'},\n",
    "                inplace=True\n",
    "            )\n",
    "\n",
    "            # transform from wide to long\n",
    "            df_wdir = pd.melt(\n",
    "                df_wdir, id_vars='lat',\n",
    "                var_name='lon',\n",
    "                value_vars=lons[:],\n",
    "                value_name='wdir'\n",
    "            )\n",
    "            \n",
    "            df_wspd = pd.melt(\n",
    "                df_wspd,\n",
    "                id_vars='lat',\n",
    "                var_name='lon',\n",
    "                value_vars=lons[:],\n",
    "                value_name='wspd'\n",
    "            )\n",
    "            \n",
    "            df_u = pd.melt(\n",
    "                df_u, id_vars='lat',\n",
    "                var_name='lon',\n",
    "                value_vars=lons[:],\n",
    "                value_name='u'\n",
    "            )\n",
    "            \n",
    "            df_v = pd.melt(\n",
    "                df_v, id_vars='lat',\n",
    "                var_name='lon',\n",
    "                value_vars=lons[:],\n",
    "                value_name='v'\n",
    "            )\n",
    "\n",
    "            # concatenate df_wdir and df_wspd\n",
    "            df_temp1 = df_wdir.merge(\n",
    "                df_wspd,\n",
    "                on=['lat', 'lon'],\n",
    "                how='left'\n",
    "            )\n",
    "            \n",
    "            # concatenate df_u and df_v\n",
    "            df_temp2 = df_u.merge(\n",
    "                df_v,\n",
    "                on=['lat', 'lon'],\n",
    "                how='left'\n",
    "            )\n",
    "            \n",
    "            # concatenate df_temp1 and df_temp2\n",
    "            df_temp = df_temp2.merge(\n",
    "                df_temp1,\n",
    "                on=['lat', 'lon'],\n",
    "                how='left'\n",
    "            )\n",
    "            \n",
    "            # add time stamp \n",
    "            df_temp['year_month'] = file.split('.')[2]\n",
    "\n",
    "            df = pd.concat(\n",
    "                [df_temp, df],\n",
    "                axis=0\n",
    "            )\n",
    "   \n",
    "    # keep values in min, max range of California geometry\n",
    "    df = df[\n",
    "        df.lon.ge(-125) & df.lon.le(-115) & df.lat.ge(32) & df.lat.le(42)\n",
    "    ]\n",
    "    \n",
    "    # transform vars\n",
    "    df['lat'] = df.lat.astype(float)\n",
    "    df['lon'] = df.lon.astype(float)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``read census geom``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_census_geom():\n",
    "    \"\"\" Read Census (lat, lon) coordinates for California zip-codes\n",
    "    parameters:\n",
    "    -----------\n",
    "    None\n",
    "    \n",
    "    return:\n",
    "    -------\n",
    "    Df with osmnx_geom\n",
    "    \"\"\"\n",
    "    ### Step 1 ### \n",
    "    ##############\n",
    "    # Read the shapefiles for California's ZIP codes\n",
    "    for file in os.listdir(in_dir_zip_shapes):\n",
    "        if file.endswith('.shp'):\n",
    "            gdf = gpd.read_file(in_dir_zip_shapes + file)\n",
    "\n",
    "    # keep only cols of interest \n",
    "    # ('ZCTA5CE10' = 2010 Census ZIP codes,\t'GEOID10' = 2010 Census Tract codes)\n",
    "    gdf = gdf[\n",
    "        ['ZCTA5CE10',\n",
    "         'GEOID10',\n",
    "         'geometry']\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    ### Step 2 ###\n",
    "    ###############\n",
    "    # For each zip cpde extract polygon with (lat, lon) info\n",
    "\n",
    "    zip_poly = pd.DataFrame()\n",
    "\n",
    "    for idx, multipoly in enumerate(gdf.geometry):\n",
    "        if isinstance(multipoly, shapely.geometry.polygon.Polygon):\n",
    "            temp_df = pd.DataFrame(\n",
    "                {\n",
    "                    'lat': multipoly.exterior.coords.xy[1], \n",
    "                    'lon': multipoly.exterior.coords.xy[0],\n",
    "                    'ZCTA10': gdf.loc[idx, 'ZCTA5CE10'],\n",
    "                    'GEOID10': gdf.loc[idx, 'GEOID10']\n",
    "                }\n",
    "            )\n",
    "            zip_poly = pd.concat(\n",
    "                [zip_poly, temp_df],\n",
    "                axis=0\n",
    "            )\n",
    "\n",
    "        if isinstance(multipoly, shapely.geometry.multipolygon.MultiPolygon):\n",
    "            for poly in multipoly:\n",
    "                temp_df = pd.DataFrame(\n",
    "                    {\n",
    "                        'lat': poly.exterior.coords.xy[1], \n",
    "                        'lon': poly.exterior.coords.xy[0],\n",
    "                        'ZCTA10': gdf.loc[idx, 'ZCTA5CE10'],\n",
    "                        'GEOID10': gdf.loc[idx, 'GEOID10']\n",
    "                    }\n",
    "                )\n",
    "                zip_poly = pd.concat(\n",
    "                    [zip_poly, temp_df],\n",
    "                    axis=0\n",
    "                )   \n",
    "    \n",
    "\n",
    "    # round (lat, lon) to 2 decimal points and add 0.005 to match the UW (lat, lon) values\n",
    "    zip_poly['lat'] = zip_poly.lat.round(3)\n",
    "    zip_poly['lon'] = zip_poly.lon.round(3)\n",
    "    \n",
    "    zip_poly.sort_values(\n",
    "        by=['ZCTA10', 'lat', 'lon'],\n",
    "        inplace=True\n",
    "    )\n",
    "    \n",
    "    zip_poly.drop_duplicates(\n",
    "        subset=['ZCTA10', 'lat', 'lon'],\n",
    "        inplace=True\n",
    "    )\n",
    "\n",
    "    zip_poly.reset_index(\n",
    "        drop=True,\n",
    "        inplace=True\n",
    "    )\n",
    "    \n",
    "    return zip_poly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``find zip (zcta) code for wind data``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_zcta_to_wind(df1, df2):\n",
    "    '''\n",
    "    params:\n",
    "    -------\n",
    "    df1: wind data\n",
    "    df2: census geometry data\n",
    "    \n",
    "    return:\n",
    "    -------\n",
    "    '''\n",
    "    \n",
    "    # create labels\n",
    "    df1['wind_lat_lon'] = [str(xy) for xy in zip(df1.lat, df1.lon)]\n",
    "    df2['census_lat_lon'] = [str(xy) for xy in zip(df2.lat, df2.lon)]\n",
    "\n",
    "    ## for each point in wind data find the nearest point in the census data ##\n",
    "    ###############\n",
    "    # keep only unique points in wind data\n",
    "    df1_unique = df1.drop_duplicates(\n",
    "        ['wind_lat_lon']\n",
    "    )\n",
    "    \n",
    "    df2_unique = df2.drop_duplicates(\n",
    "        ['census_lat_lon']\n",
    "    )\n",
    "    \n",
    "    df1_unique.reset_index(\n",
    "        drop=True,\n",
    "        inplace=True\n",
    "    )\n",
    "    \n",
    "    df2_unique.reset_index(\n",
    "        drop=True,\n",
    "        inplace=True\n",
    "    )\n",
    "\n",
    "    # transform to radians\n",
    "    df1_unique['lat_r'] = np.radians(df1_unique.lat)\n",
    "    df1_unique['lon_r'] = np.radians(df1_unique.lon)\n",
    "    df2_unique['lat_r'] = np.radians(df2_unique.lat)\n",
    "    df2_unique['lon_r'] = np.radians(df2_unique.lon)\n",
    "\n",
    "\n",
    "    # compute pairwise distance (in miles)\n",
    "    dist_matrix = (dist.pairwise(\n",
    "        df2_unique[['lat_r', 'lon_r']],\n",
    "        df1_unique[['lat_r', 'lon_r']]\n",
    "    ))*3959\n",
    "\n",
    "    # create a df from dist_matrix\n",
    "    dist_matrix = pd.DataFrame(\n",
    "        dist_matrix,\n",
    "        index=df2_unique['census_lat_lon'],\n",
    "        columns=df1_unique['wind_lat_lon']\n",
    "    )\n",
    "    \n",
    "    # for each row (census_lat_lon point) extract the closest column (wind_lat_lon point) \n",
    "    closest_point = pd.DataFrame(\n",
    "        dist_matrix.idxmin(axis=1),\n",
    "        columns=['closest_wind_lat_lon']\n",
    "    )\n",
    "    \n",
    "    closest_point.reset_index(\n",
    "        drop=False,\n",
    "        inplace=True\n",
    "    )\n",
    "\n",
    "    # merge with census data\n",
    "    df2_unique = df2_unique.merge(\n",
    "        closest_point,\n",
    "        on='census_lat_lon',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # merge with census data \n",
    "    df2_unique = df2_unique.merge(\n",
    "        df2[['census_lat_lon']],\n",
    "        on=['census_lat_lon'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # replicate df2_unique based on number of year_month entries in df1\n",
    "    df2_unique = pd.concat(\n",
    "        [df2_unique]*(df1.year_month.nunique()),\n",
    "        axis=0\n",
    "    )\n",
    "    \n",
    "    df2_unique.reset_index(\n",
    "        drop=True,\n",
    "        inplace=True\n",
    "    )\n",
    "    \n",
    "    # add year_month column to df2_unique\n",
    "    df2_unique['year_month'] = 0\n",
    "    indeces = [n for n in range(1, df2_unique.shape[0]) if n%956926==0]\n",
    "\n",
    "    year_month = np.sort(df1.year_month.unique())\n",
    "    for idx, index in enumerate(indeces):\n",
    "        if idx==0:\n",
    "            df2_unique.iloc[0:indeces[idx], 8] = year_month[idx]\n",
    "        else:\n",
    "            df2_unique.iloc[indeces[idx-1]:indeces[idx], 8] = year_month[idx]\n",
    "            \n",
    "            \n",
    "    # from df1 keep only cols of interest\n",
    "    df1 = df1[\n",
    "        ['year_month',\n",
    "         'u',\n",
    "         'v',\n",
    "         'wdir',\n",
    "         'wspd',\n",
    "         'wind_lat_lon']\n",
    "    ]\n",
    "    \n",
    "    # merge df2_unique with df1\n",
    "    df2_unique = df2_unique.merge(\n",
    "        df1,\n",
    "        left_on=['year_month', 'closest_wind_lat_lon'],\n",
    "        right_on=['year_month', 'wind_lat_lon'],\n",
    "        how='left'\n",
    "    )\n",
    "    # keep only cols of interest\n",
    "    df2_unique = df2_unique[\n",
    "        ['lat',\n",
    "         'lon',\n",
    "         'ZCTA10',\n",
    "         'u',\n",
    "         'v',\n",
    "         'wdir',\n",
    "         'wspd',\n",
    "         'year_month']\n",
    "    ]\n",
    "    \n",
    "    df2_unique.dropna(\n",
    "        inplace=True\n",
    "    )\n",
    "    \n",
    "    df2_unique.reset_index(\n",
    "        drop=True,\n",
    "        inplace=True\n",
    "    )\n",
    "    \n",
    "    df2_unique.drop_duplicates(\n",
    "    ['year_month', 'ZCTA10'],\n",
    "    inplace=True\n",
    "    )\n",
    "\n",
    "    df2_unique.reset_index(\n",
    "        drop=True,\n",
    "        inplace=True\n",
    "    )\n",
    "    \n",
    "    return df2_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_poly = read_census_geom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- File 1991winds.csv already exists. -\n",
      "- File 1992winds.csv already exists. -\n",
      "- File 1993winds.csv already exists. -\n",
      "- File 1994winds.csv already exists. -\n",
      "- File 1995winds.csv already exists. -\n",
      "- File 1996winds.csv already exists. -\n",
      "- File 1997winds.csv already exists. -\n",
      "- File 1998winds.csv already exists. -\n",
      "- File 1999winds.csv already exists. -\n",
      "- File 2000winds.csv already exists. -\n",
      "- File 2001winds.csv already exists. -\n",
      "- File 2002winds.csv already exists. -\n",
      "- File 2003winds.csv already exists. -\n",
      "- File 2004winds.csv already exists. -\n",
      "- File 2005winds.csv already exists. -\n",
      "- File 2006winds.csv already exists. -\n",
      "- File 2007winds.csv already exists. -\n",
      "- File 2008winds.csv already exists. -\n",
      "- File 2009winds.csv already exists. -\n",
      "- File 2010winds.csv already exists. -\n",
      "- File 2011winds.csv already exists. -\n",
      "- File 2012winds.csv already exists. -\n",
      "- File 2013winds.csv already exists. -\n",
      "- File 2014winds.csv already exists. -\n",
      "- File 2015winds.csv already exists. -\n",
      "- File 2016winds.csv already exists. -\n",
      "- File 2017winds.csv already exists. -\n",
      "- File 2018winds.csv already exists. -\n",
      "- File 2019winds.csv already exists. -\n",
      "- File 2020winds.csv already exists. -\n",
      "- File 2021winds.csv already exists. -\n",
      "- File 2022winds.csv already exists. -\n"
     ]
    }
   ],
   "source": [
    "for year in range(1991,2023):\n",
    "    if os.path.exists(out_dir + str(year) + 'winds.csv'):\n",
    "        print('- File ' + str(year) + 'winds.csv already exists. -')\n",
    "    else:\n",
    "        print(f'----- Starting on year {year} -----')\n",
    "        df = read_clean_wind(year)\n",
    "        df_final = add_zcta_to_wind(df, zip_poly)\n",
    "        df_final.to_csv(out_dir  + str(year) + 'winds.csv')\n",
    "        print('--- Now saving file' + out_dir  + str(year) + 'winds.csv ---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``census geom``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "files = glob.glob(out_dir+\"*.csv\")\n",
    "\n",
    "df_all_years = pd.DataFrame()\n",
    "for f in files:\n",
    "    csv = pd.read_csv(f)\n",
    "    df_all_years = df_all_years.append(csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>ZCTA10</th>\n",
       "      <th>u</th>\n",
       "      <th>v</th>\n",
       "      <th>wdir</th>\n",
       "      <th>wspd</th>\n",
       "      <th>year_month</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>37.465</td>\n",
       "      <td>-117.936</td>\n",
       "      <td>89010</td>\n",
       "      <td>0.992558</td>\n",
       "      <td>0.124684</td>\n",
       "      <td>7.159901</td>\n",
       "      <td>1.000358</td>\n",
       "      <td>200601</td>\n",
       "      <td>2006</td>\n",
       "      <td>01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>35.396</td>\n",
       "      <td>-116.322</td>\n",
       "      <td>89019</td>\n",
       "      <td>0.088867</td>\n",
       "      <td>-0.147450</td>\n",
       "      <td>301.077087</td>\n",
       "      <td>0.172160</td>\n",
       "      <td>200601</td>\n",
       "      <td>2006</td>\n",
       "      <td>01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>36.161</td>\n",
       "      <td>-116.139</td>\n",
       "      <td>89060</td>\n",
       "      <td>-0.319009</td>\n",
       "      <td>-0.046086</td>\n",
       "      <td>188.220367</td>\n",
       "      <td>0.322321</td>\n",
       "      <td>200601</td>\n",
       "      <td>2006</td>\n",
       "      <td>01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>35.957</td>\n",
       "      <td>-115.897</td>\n",
       "      <td>89061</td>\n",
       "      <td>-0.106072</td>\n",
       "      <td>-0.459862</td>\n",
       "      <td>257.011322</td>\n",
       "      <td>0.471937</td>\n",
       "      <td>200601</td>\n",
       "      <td>2006</td>\n",
       "      <td>01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>39.520</td>\n",
       "      <td>-120.032</td>\n",
       "      <td>89439</td>\n",
       "      <td>0.707161</td>\n",
       "      <td>0.798291</td>\n",
       "      <td>48.464073</td>\n",
       "      <td>1.066463</td>\n",
       "      <td>200601</td>\n",
       "      <td>2006</td>\n",
       "      <td>01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19591</th>\n",
       "      <td>19591</td>\n",
       "      <td>39.061</td>\n",
       "      <td>-120.210</td>\n",
       "      <td>96145</td>\n",
       "      <td>0.370858</td>\n",
       "      <td>0.799831</td>\n",
       "      <td>65.124237</td>\n",
       "      <td>0.881626</td>\n",
       "      <td>199412</td>\n",
       "      <td>1994</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19592</th>\n",
       "      <td>19592</td>\n",
       "      <td>39.149</td>\n",
       "      <td>-120.248</td>\n",
       "      <td>96146</td>\n",
       "      <td>0.370858</td>\n",
       "      <td>0.799831</td>\n",
       "      <td>65.124237</td>\n",
       "      <td>0.881626</td>\n",
       "      <td>199412</td>\n",
       "      <td>1994</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19593</th>\n",
       "      <td>19593</td>\n",
       "      <td>39.236</td>\n",
       "      <td>-120.062</td>\n",
       "      <td>96148</td>\n",
       "      <td>0.370858</td>\n",
       "      <td>0.799831</td>\n",
       "      <td>65.124237</td>\n",
       "      <td>0.881626</td>\n",
       "      <td>199412</td>\n",
       "      <td>1994</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19594</th>\n",
       "      <td>19594</td>\n",
       "      <td>38.732</td>\n",
       "      <td>-120.033</td>\n",
       "      <td>96150</td>\n",
       "      <td>-0.001801</td>\n",
       "      <td>0.435539</td>\n",
       "      <td>90.236954</td>\n",
       "      <td>0.435543</td>\n",
       "      <td>199412</td>\n",
       "      <td>1994</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19595</th>\n",
       "      <td>19595</td>\n",
       "      <td>39.184</td>\n",
       "      <td>-120.427</td>\n",
       "      <td>96161</td>\n",
       "      <td>-0.250691</td>\n",
       "      <td>0.245439</td>\n",
       "      <td>135.606461</td>\n",
       "      <td>0.350837</td>\n",
       "      <td>199412</td>\n",
       "      <td>1994</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>627072 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0     lat      lon  ZCTA10         u         v        wdir  \\\n",
       "0               0  37.465 -117.936   89010  0.992558  0.124684    7.159901   \n",
       "1               1  35.396 -116.322   89019  0.088867 -0.147450  301.077087   \n",
       "2               2  36.161 -116.139   89060 -0.319009 -0.046086  188.220367   \n",
       "3               3  35.957 -115.897   89061 -0.106072 -0.459862  257.011322   \n",
       "4               4  39.520 -120.032   89439  0.707161  0.798291   48.464073   \n",
       "...           ...     ...      ...     ...       ...       ...         ...   \n",
       "19591       19591  39.061 -120.210   96145  0.370858  0.799831   65.124237   \n",
       "19592       19592  39.149 -120.248   96146  0.370858  0.799831   65.124237   \n",
       "19593       19593  39.236 -120.062   96148  0.370858  0.799831   65.124237   \n",
       "19594       19594  38.732 -120.033   96150 -0.001801  0.435539   90.236954   \n",
       "19595       19595  39.184 -120.427   96161 -0.250691  0.245439  135.606461   \n",
       "\n",
       "           wspd year_month  year month  \n",
       "0      1.000358     200601  2006    01  \n",
       "1      0.172160     200601  2006    01  \n",
       "2      0.322321     200601  2006    01  \n",
       "3      0.471937     200601  2006    01  \n",
       "4      1.066463     200601  2006    01  \n",
       "...         ...        ...   ...   ...  \n",
       "19591  0.881626     199412  1994    12  \n",
       "19592  0.881626     199412  1994    12  \n",
       "19593  0.881626     199412  1994    12  \n",
       "19594  0.435543     199412  1994    12  \n",
       "19595  0.350837     199412  1994    12  \n",
       "\n",
       "[627072 rows x 11 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_years['year_month']=df_all_years['year_month'].astype(str)\n",
    "df_all_years['year']=df_all_years['year_month'].str.slice(start=0,stop=4)\n",
    "df_all_years['month']=df_all_years['year_month'].str.slice(start=4,stop=6)\n",
    "\n",
    "df_all_years.to_csv(out_dir+\"all_years_wind_data.csv\")\n",
    "df_all_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

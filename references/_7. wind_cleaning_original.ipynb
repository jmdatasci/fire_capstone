{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main script to clean wind data at the zip code, monthly level\n",
    "\n",
    "Modules: N/A <br>\n",
    "Author: Cornelia Ilin <br>\n",
    "Email: cilin@wisc.edu <br>\n",
    "Date created: May 14, 2021 <br>\n",
    "\n",
    "**Citations (data sources)**\n",
    "\n",
    "``Wind data:`` \n",
    "\n",
    "download the \"MERRA2_100.tavgM_2d_slv_Nx\" product; this provides monthly averages of U and V components\n",
    "\n",
    "1. https://search.earthdata.nasa.gov/search/granules?p=C1276812859-GES_DISC&pg[0][qt]=1991-01-01T00%3A00%3A00.000Z%2C2017-12-31T23%3A59%3A59.999Z&pg[0][gsk]=-start_date&q=MERRA-2%20tavgM&tl=1624239533!3!!&m=-0.0703125!0.0703125!2!1!0!0%2C2\n",
    "\n",
    "and data dictionary here:\n",
    "\n",
    "2. https://gmao.gsfc.nasa.gov/pubs/docs/Bosilovich785.pdf\n",
    "3. https://disc.gsfc.nasa.gov/datasets/M2T1NXSLV_5.12.4/summary\n",
    "\n",
    "\n",
    "``Shapefiles for California ZIP codes (2010 census):``\n",
    "\n",
    "4. https://www.census.gov/cgi-bin/geo/shapefiles/index.php?year=2010&layergroup=ZIP+Code+Tabulation+Areas\n",
    "\n",
    "``Installation errors with Geopandas:``\n",
    "\n",
    "5. https://stackoverflow.com/questions/54734667/error-installing-geopandas-a-gdal-api-version-must-be-specified-in-anaconda\n",
    "\n",
    "``How to compute wind speed and direction:``\n",
    "\n",
    "6. https://stackoverflow.com/questions/21484558/how-to-calculate-wind-direction-from-u-and-v-wind-components-in-r\n",
    "7. https://github.com/blaylockbk/Ute_WRF/blob/master/functions/wind_calcs.py\n",
    "\n",
    "``Wind speed and direction intuition:``\n",
    "\n",
    "8. http://colaweb.gmu.edu/dev/clim301/lectures/wind/wind-uv\n",
    "9. https://www.earthdatascience.org/courses/use-data-open-source-python/intro-vector-data-python/spatial-data-vector-shapefiles/intro-to-coordinate-reference-systems-python/\n",
    "\n",
    "``To create maps of this wind data:``\n",
    "\n",
    "and also used to provide intuition for winddir and windspeed\n",
    "\n",
    "10. https://disc.gsfc.nasa.gov/information/howto?title=How%20to%20calculate%20and%20plot%20wind%20speed%20using%20MERRA-2%20wind%20component%20data%20using%20Python\n",
    "\n",
    "\n",
    "\n",
    "**Citations (persons)**\n",
    "1. N/A\n",
    "\n",
    "**Preferred environment**\n",
    "1. Code written in Jupyter Notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'netCDF4'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnetCDF4\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mncdf\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m date, timedelta\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'netCDF4'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import netCDF4 as ncdf\n",
    "import os\n",
    "from datetime import date, timedelta\n",
    "from math import pi\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "import matplotlib.ticker as mticker\n",
    "\n",
    "# geography\n",
    "import geopandas as gpd\n",
    "import osmnx as ox\n",
    "import shapely\n",
    "from shapely.geometry import Point\n",
    "import sklearn.neighbors\n",
    "dist = sklearn.neighbors.DistanceMetric.get_metric(\n",
    "    'haversine'\n",
    ")\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    'ignore'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define working directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dir_zip_shapes = 'C:/Users/cilin/Research/CA_hospitals/Input/raw_data/census_geo/shapefiles_zcta/'\n",
    "in_dir = 'C:/Users/cilin/Research/CA_hospitals/Input/raw_data/winds/'\n",
    "in_health = 'C:/Users/cilin/Research/CA_hospitals/Input/final_data/health/'\n",
    "out_dir = 'C:/Users/cilin/Research/CA_hospitals/Input/final_data/winds/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``read_clean wind``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_clean_wind():\n",
    "    ''''''\n",
    "    # create empty df\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    for file in os.listdir(in_dir):\n",
    "        if file.startswith('MERRA2'):\n",
    "            print(file.split('.')[2])\n",
    "\n",
    "            ## read .nc file ##\n",
    "            ###################\n",
    "            data = ncdf.Dataset(\n",
    "                in_dir + file, mode='r'\n",
    "            )\n",
    "            # print metadata\n",
    "            #print(data)\n",
    "\n",
    "            # grab vars of interest ##\n",
    "            ##########################\n",
    "            # longitude and latitude\n",
    "            lons = data.variables['lon']\n",
    "            lats = data.variables['lat']\n",
    "            # 2-meter eastward wind m/s\n",
    "            U2M = data.variables['U2M']\n",
    "            # 2-meter northward wind m/s\n",
    "            V2M = data.variables['V2M']\n",
    "\n",
    "            # Replace vals #\n",
    "            ################\n",
    "            #\\_FillValues with NaNs:\n",
    "            U2M_nans = U2M[:]\n",
    "            V2M_nans = V2M[:]\n",
    "            _FillValueU2M = U2M._FillValue\n",
    "            _FillValueV2M = V2M._FillValue\n",
    "            U2M_nans[U2M_nans == _FillValueU2M] = np.nan\n",
    "            V2M_nans[V2M_nans == _FillValueV2M] = np.nan\n",
    "\n",
    "            # Add new vars #\n",
    "            ################\n",
    "            # calculate wind speed\n",
    "            wspd = np.sqrt(U2M_nans**2+V2M_nans**2)\n",
    "\n",
    "            # calculate wind direction in radians\n",
    "            wdir = np.arctan2(V2M_nans, U2M_nans)\n",
    "            \n",
    "            # transform wind direction from radians to degrees\n",
    "            #dir_to_degrees = np.mod(180+np.rad2deg(np.arctan2(V2M_nans, U2M_nans)), 360) # this computes \"wind is blowing from\"' meteorological convetion'\n",
    "            wdir_to_degrees = np.mod(np.rad2deg(wdir), 360) # this computes \"wind is blowing towards\" 'oceonographic convention', see here: https://www.esri.com/arcgis-blog/products/product/analytics/displaying-speed-and-direction-symbology-from-u-and-v-vectors/\n",
    "            \n",
    "            \n",
    "            ## transform to df ##\n",
    "            #####################\n",
    "            # create an empty df for wind speed and direction with size len(lats) x len(lons) \n",
    "            df_wdir = pd.DataFrame(index=lats[:], columns=lons[:])   \n",
    "            df_wspd = pd.DataFrame(index=lats[:], columns=lons[:])\n",
    "            \n",
    "            # create an empty df for u and v components with size len(lats) x len(lons) \n",
    "            df_u = pd.DataFrame(index=lats[:], columns=lons[:])\n",
    "            df_v = pd.DataFrame(index=lats[:], columns=lons[:])\n",
    "\n",
    "            # populate each row in the empty df above with the wdir_meteo and wspd data and u and v components\n",
    "            for idx, idx_val in enumerate(df_wdir.index):\n",
    "                df_wdir.loc[idx_val, :] = wdir_to_degrees[0][idx]\n",
    "                df_wspd.loc[idx_val, :] = wspd[0][idx]\n",
    "                df_u.loc[idx_val, :] = U2M_nans[0][idx]\n",
    "                df_v.loc[idx_val, :] = V2M_nans[0][idx]\n",
    "\n",
    "            # add index (latitude) as column\n",
    "            df_wdir.reset_index(\n",
    "                drop=False,\n",
    "                inplace=True\n",
    "            )\n",
    "            \n",
    "            df_wdir.rename(\n",
    "                columns={'index':'lat'},\n",
    "                inplace=True\n",
    "            )\n",
    "            \n",
    "            \n",
    "            df_wspd.reset_index(\n",
    "                drop=False,\n",
    "                inplace=True\n",
    "            )\n",
    "            \n",
    "            df_wspd.rename(\n",
    "                columns={'index':'lat'},\n",
    "                inplace=True\n",
    "            )\n",
    "            \n",
    "            df_u.reset_index(\n",
    "                drop=False,\n",
    "                inplace=True\n",
    "            )\n",
    "            \n",
    "            df_u.rename(\n",
    "                columns={'index':'lat'},\n",
    "                inplace=True\n",
    "            )\n",
    "            \n",
    "            df_v.reset_index(\n",
    "                drop=False,\n",
    "                inplace=True\n",
    "            )\n",
    "            \n",
    "            df_v.rename(\n",
    "                columns={'index':'lat'},\n",
    "                inplace=True\n",
    "            )\n",
    "\n",
    "            # transform from wide to long\n",
    "            df_wdir = pd.melt(\n",
    "                df_wdir, id_vars='lat',\n",
    "                var_name='lon',\n",
    "                value_vars=lons[:],\n",
    "                value_name='wdir'\n",
    "            )\n",
    "            \n",
    "            df_wspd = pd.melt(\n",
    "                df_wspd,\n",
    "                id_vars='lat',\n",
    "                var_name='lon',\n",
    "                value_vars=lons[:],\n",
    "                value_name='wspd'\n",
    "            )\n",
    "            \n",
    "            df_u = pd.melt(\n",
    "                df_u, id_vars='lat',\n",
    "                var_name='lon',\n",
    "                value_vars=lons[:],\n",
    "                value_name='u'\n",
    "            )\n",
    "            \n",
    "            df_v = pd.melt(\n",
    "                df_v, id_vars='lat',\n",
    "                var_name='lon',\n",
    "                value_vars=lons[:],\n",
    "                value_name='v'\n",
    "            )\n",
    "\n",
    "            # concatenate df_wdir and df_wspd\n",
    "            df_temp1 = df_wdir.merge(\n",
    "                df_wspd,\n",
    "                on=['lat', 'lon'],\n",
    "                how='left'\n",
    "            )\n",
    "            \n",
    "            # concatenate df_u and df_v\n",
    "            df_temp2 = df_u.merge(\n",
    "                df_v,\n",
    "                on=['lat', 'lon'],\n",
    "                how='left'\n",
    "            )\n",
    "            \n",
    "            # concatenate df_temp1 and df_temp2\n",
    "            df_temp = df_temp2.merge(\n",
    "                df_temp1,\n",
    "                on=['lat', 'lon'],\n",
    "                how='left'\n",
    "            )\n",
    "            \n",
    "            # add time stamp \n",
    "            df_temp['year_month'] = file.split('.')[2]\n",
    "\n",
    "            df = pd.concat(\n",
    "                [df_temp, df],\n",
    "                axis=0\n",
    "            )\n",
    "   \n",
    "    # keep values in min, max range of California geometry\n",
    "    df = df[\n",
    "        df.lon.ge(-125) & df.lon.le(-115) & df.lat.ge(32) & df.lat.le(42)\n",
    "    ]\n",
    "    \n",
    "    # transform vars\n",
    "    df['lat'] = df.lat.astype(float)\n",
    "    df['lon'] = df.lon.astype(float)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``read census geom``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_census_geom():\n",
    "    \"\"\" Read Census (lat, lon) coordinates for California zip-codes\n",
    "    parameters:\n",
    "    -----------\n",
    "    None\n",
    "    \n",
    "    return:\n",
    "    -------\n",
    "    Df with osmnx_geom\n",
    "    \"\"\"\n",
    "    ### Step 1 ### \n",
    "    ##############\n",
    "    # Read the shapefiles for California's ZIP codes\n",
    "    for file in os.listdir(in_dir_zip_shapes):\n",
    "        if file.endswith('.shp'):\n",
    "            gdf = gpd.read_file(in_dir_zip_shapes + file)\n",
    "\n",
    "    # keep only cols of interest \n",
    "    # ('ZCTA5CE10' = 2010 Census ZIP codes,\t'GEOID10' = 2010 Census Tract codes)\n",
    "    gdf = gdf[\n",
    "        ['ZCTA5CE10',\n",
    "         'GEOID10',\n",
    "         'geometry']\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    ### Step 2 ###\n",
    "    ###############\n",
    "    # For each zip cpde extract polygon with (lat, lon) info\n",
    "\n",
    "    zip_poly = pd.DataFrame()\n",
    "\n",
    "    for idx, multipoly in enumerate(gdf.geometry):\n",
    "        if isinstance(multipoly, shapely.geometry.polygon.Polygon):\n",
    "            temp_df = pd.DataFrame(\n",
    "                {\n",
    "                    'lat': multipoly.exterior.coords.xy[1], \n",
    "                    'lon': multipoly.exterior.coords.xy[0],\n",
    "                    'ZCTA10': gdf.loc[idx, 'ZCTA5CE10'],\n",
    "                    'GEOID10': gdf.loc[idx, 'GEOID10']\n",
    "                }\n",
    "            )\n",
    "            zip_poly = pd.concat(\n",
    "                [zip_poly, temp_df],\n",
    "                axis=0\n",
    "            )\n",
    "\n",
    "        if isinstance(multipoly, shapely.geometry.multipolygon.MultiPolygon):\n",
    "            for poly in multipoly:\n",
    "                temp_df = pd.DataFrame(\n",
    "                    {\n",
    "                        'lat': poly.exterior.coords.xy[1], \n",
    "                        'lon': poly.exterior.coords.xy[0],\n",
    "                        'ZCTA10': gdf.loc[idx, 'ZCTA5CE10'],\n",
    "                        'GEOID10': gdf.loc[idx, 'GEOID10']\n",
    "                    }\n",
    "                )\n",
    "                zip_poly = pd.concat(\n",
    "                    [zip_poly, temp_df],\n",
    "                    axis=0\n",
    "                )   \n",
    "    \n",
    "\n",
    "    # round (lat, lon) to 2 decimal points and add 0.005 to match the UW (lat, lon) values\n",
    "    zip_poly['lat'] = zip_poly.lat.round(3)\n",
    "    zip_poly['lon'] = zip_poly.lon.round(3)\n",
    "    \n",
    "    zip_poly.sort_values(\n",
    "        by=['ZCTA10', 'lat', 'lon'],\n",
    "        inplace=True\n",
    "    )\n",
    "    \n",
    "    zip_poly.drop_duplicates(\n",
    "        subset=['ZCTA10', 'lat', 'lon'],\n",
    "        inplace=True\n",
    "    )\n",
    "\n",
    "    zip_poly.reset_index(\n",
    "        drop=True,\n",
    "        inplace=True\n",
    "    )\n",
    "    \n",
    "    return zip_poly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``find zip (zcta) code for wind data``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_zcta_to_wind(df1, df2):\n",
    "    '''\n",
    "    params:\n",
    "    -------\n",
    "    df1: wind data\n",
    "    df2: census geometry data\n",
    "    \n",
    "    return:\n",
    "    -------\n",
    "    '''\n",
    "    \n",
    "    # create labels\n",
    "    df1['wind_lat_lon'] = [str(xy) for xy in zip(df1.lat, df1.lon)]\n",
    "    df2['census_lat_lon'] = [str(xy) for xy in zip(df2.lat, df2.lon)]\n",
    "\n",
    "    ## for each point in wind data find the nearest point in the census data ##\n",
    "    ###############\n",
    "    # keep only unique points in wind data\n",
    "    df1_unique = df1.drop_duplicates(\n",
    "        ['wind_lat_lon']\n",
    "    )\n",
    "    \n",
    "    df2_unique = df2.drop_duplicates(\n",
    "        ['census_lat_lon']\n",
    "    )\n",
    "    \n",
    "    df1_unique.reset_index(\n",
    "        drop=True,\n",
    "        inplace=True\n",
    "    )\n",
    "    \n",
    "    df2_unique.reset_index(\n",
    "        drop=True,\n",
    "        inplace=True\n",
    "    )\n",
    "\n",
    "    # transform to radians\n",
    "    df1_unique['lat_r'] = np.radians(df1_unique.lat)\n",
    "    df1_unique['lon_r'] = np.radians(df1_unique.lon)\n",
    "    df2_unique['lat_r'] = np.radians(df2_unique.lat)\n",
    "    df2_unique['lon_r'] = np.radians(df2_unique.lon)\n",
    "\n",
    "\n",
    "    # compute pairwise distance (in miles)\n",
    "    dist_matrix = (dist.pairwise(\n",
    "        df2_unique[['lat_r', 'lon_r']],\n",
    "        df1_unique[['lat_r', 'lon_r']]\n",
    "    ))*3959\n",
    "\n",
    "    # create a df from dist_matrix\n",
    "    dist_matrix = pd.DataFrame(\n",
    "        dist_matrix,\n",
    "        index=df2_unique['census_lat_lon'],\n",
    "        columns=df1_unique['wind_lat_lon']\n",
    "    )\n",
    "    \n",
    "    # for each row (census_lat_lon point) extract the closest column (wind_lat_lon point) \n",
    "    closest_point = pd.DataFrame(\n",
    "        dist_matrix.idxmin(axis=1),\n",
    "        columns=['closest_wind_lat_lon']\n",
    "    )\n",
    "    \n",
    "    closest_point.reset_index(\n",
    "        drop=False,\n",
    "        inplace=True\n",
    "    )\n",
    "\n",
    "    # merge with census data\n",
    "    df2_unique = df2_unique.merge(\n",
    "        closest_point,\n",
    "        on='census_lat_lon',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # merge with census data \n",
    "    df2_unique = df2_unique.merge(\n",
    "        df2[['census_lat_lon']],\n",
    "        on=['census_lat_lon'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # replicate df2_unique based on number of year_month entries in df1\n",
    "    df2_unique = pd.concat(\n",
    "        [df2_unique]*(df1.year_month.nunique()),\n",
    "        axis=0\n",
    "    )\n",
    "    \n",
    "    df2_unique.reset_index(\n",
    "        drop=True,\n",
    "        inplace=True\n",
    "    )\n",
    "    \n",
    "    # add year_month column to df2_unique\n",
    "    df2_unique['year_month'] = 0\n",
    "    indeces = [n for n in range(1, df2_unique.shape[0]) if n%956926==0]\n",
    "\n",
    "    year_month = np.sort(df1.year_month.unique())\n",
    "    for idx, index in enumerate(indeces):\n",
    "        if idx==0:\n",
    "            df2_unique.iloc[0:indeces[idx], 8] = year_month[idx]\n",
    "        else:\n",
    "            df2_unique.iloc[indeces[idx-1]:indeces[idx], 8] = year_month[idx]\n",
    "            \n",
    "            \n",
    "    # from df1 keep only cols of interest\n",
    "    df1 = df1[\n",
    "        ['year_month',\n",
    "         'u',\n",
    "         'v',\n",
    "         'wdir',\n",
    "         'wspd',\n",
    "         'wind_lat_lon']\n",
    "    ]\n",
    "    \n",
    "    # merge df2_unique with df1\n",
    "    df2_unique = df2_unique.merge(\n",
    "        df1,\n",
    "        left_on=['year_month', 'closest_wind_lat_lon'],\n",
    "        right_on=['year_month', 'wind_lat_lon'],\n",
    "        how='left'\n",
    "    )\n",
    "    # keep only cols of interest\n",
    "    df2_unique = df2_unique[\n",
    "        ['lat',\n",
    "         'lon',\n",
    "         'ZCTA10',\n",
    "         'u',\n",
    "         'v',\n",
    "         'wdir',\n",
    "         'wspd',\n",
    "         'year_month']\n",
    "    ]\n",
    "    \n",
    "    df2_unique.dropna(\n",
    "        inplace=True\n",
    "    )\n",
    "    \n",
    "    df2_unique.reset_index(\n",
    "        drop=True,\n",
    "        inplace=True\n",
    "    )\n",
    "    \n",
    "    df2_unique.drop_duplicates(\n",
    "    ['year_month', 'ZCTA10'],\n",
    "    inplace=True\n",
    "    )\n",
    "\n",
    "    df2_unique.reset_index(\n",
    "        drop=True,\n",
    "        inplace=True\n",
    "    )\n",
    "    \n",
    "    return df2_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Read data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``wind``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199101\n",
      "199102\n",
      "199103\n",
      "199104\n",
      "199105\n",
      "199106\n",
      "199107\n",
      "199108\n",
      "199109\n",
      "199110\n",
      "199111\n",
      "199112\n",
      "199201\n",
      "199202\n",
      "199203\n",
      "199204\n",
      "199205\n",
      "199206\n",
      "199207\n",
      "199208\n",
      "199209\n",
      "199210\n",
      "199211\n",
      "199212\n",
      "199301\n",
      "199302\n",
      "199303\n",
      "199304\n",
      "199305\n",
      "199306\n",
      "199307\n",
      "199308\n",
      "199309\n",
      "199310\n",
      "199311\n",
      "199312\n",
      "199401\n",
      "199402\n",
      "199403\n",
      "199404\n",
      "199405\n",
      "199406\n",
      "199407\n",
      "199408\n",
      "199409\n",
      "199410\n",
      "199411\n",
      "199412\n",
      "199501\n",
      "199502\n",
      "199503\n",
      "199504\n",
      "199505\n",
      "199506\n",
      "199507\n",
      "199508\n",
      "199509\n",
      "199510\n",
      "199511\n",
      "199512\n",
      "199601\n",
      "199602\n",
      "199603\n",
      "199604\n",
      "199605\n",
      "199606\n",
      "199607\n",
      "199608\n",
      "199609\n",
      "199610\n",
      "199611\n",
      "199612\n",
      "199701\n",
      "199702\n",
      "199703\n",
      "199704\n",
      "199705\n",
      "199706\n",
      "199707\n",
      "199708\n",
      "199709\n",
      "199710\n",
      "199711\n",
      "199712\n",
      "199801\n",
      "199802\n",
      "199803\n",
      "199804\n",
      "199805\n",
      "199806\n",
      "199807\n",
      "199808\n",
      "199809\n",
      "199810\n",
      "199811\n",
      "199812\n",
      "199901\n",
      "199902\n",
      "199903\n",
      "199904\n",
      "199905\n",
      "199906\n",
      "199907\n",
      "199908\n",
      "199909\n",
      "199910\n",
      "199911\n",
      "199912\n",
      "200001\n",
      "200002\n",
      "200003\n",
      "200004\n",
      "200005\n",
      "200006\n",
      "200007\n",
      "200008\n",
      "200009\n",
      "200010\n",
      "200011\n",
      "200012\n",
      "200101\n",
      "200102\n",
      "200103\n",
      "200104\n",
      "200105\n",
      "200106\n",
      "200107\n",
      "200108\n",
      "200109\n",
      "200110\n",
      "200111\n",
      "200112\n",
      "200201\n",
      "200202\n",
      "200203\n",
      "200204\n",
      "200205\n",
      "200206\n",
      "200207\n",
      "200208\n",
      "200209\n",
      "200210\n",
      "200211\n",
      "200212\n",
      "200301\n",
      "200302\n",
      "200303\n",
      "200304\n",
      "200305\n",
      "200306\n",
      "200307\n",
      "200308\n",
      "200309\n",
      "200310\n",
      "200311\n",
      "200312\n",
      "200401\n",
      "200402\n",
      "200403\n",
      "200404\n",
      "200405\n",
      "200406\n",
      "200407\n",
      "200408\n",
      "200409\n",
      "200410\n",
      "200411\n",
      "200412\n",
      "200501\n",
      "200502\n",
      "200503\n",
      "200504\n",
      "200505\n",
      "200506\n",
      "200507\n",
      "200508\n",
      "200509\n",
      "200510\n",
      "200511\n",
      "200512\n",
      "200601\n",
      "200602\n",
      "200603\n",
      "200604\n",
      "200605\n",
      "200606\n",
      "200607\n",
      "200608\n",
      "200609\n",
      "200610\n",
      "200611\n",
      "200612\n",
      "200701\n",
      "200702\n",
      "200703\n",
      "200704\n",
      "200705\n",
      "200706\n",
      "200707\n",
      "200708\n",
      "200709\n",
      "200710\n",
      "200711\n",
      "200712\n",
      "200801\n",
      "200802\n",
      "200803\n",
      "200804\n",
      "200805\n",
      "200806\n",
      "200807\n",
      "200808\n",
      "200809\n",
      "200810\n",
      "200811\n",
      "200812\n",
      "200901\n",
      "200902\n",
      "200903\n",
      "200904\n",
      "200905\n",
      "200906\n",
      "200907\n",
      "200908\n",
      "200909\n",
      "200910\n",
      "200911\n",
      "200912\n",
      "201001\n",
      "201002\n",
      "201003\n",
      "201004\n",
      "201005\n",
      "201006\n",
      "201007\n",
      "201008\n",
      "201009\n",
      "201010\n",
      "201011\n",
      "201012\n",
      "201101\n",
      "201102\n",
      "201103\n",
      "201104\n",
      "201105\n",
      "201106\n",
      "201107\n",
      "201108\n",
      "201109\n",
      "201110\n",
      "201111\n",
      "201112\n",
      "201201\n",
      "201202\n",
      "201203\n",
      "201204\n",
      "201205\n",
      "201206\n",
      "201207\n",
      "201208\n",
      "201209\n",
      "201210\n",
      "201211\n",
      "201212\n",
      "201301\n",
      "201302\n",
      "201303\n",
      "201304\n",
      "201305\n",
      "201306\n",
      "201307\n",
      "201308\n",
      "201309\n",
      "201310\n",
      "201311\n",
      "201312\n",
      "201401\n",
      "201402\n",
      "201403\n",
      "201404\n",
      "201405\n",
      "201406\n",
      "201407\n",
      "201408\n",
      "201409\n",
      "201410\n",
      "201411\n",
      "201412\n",
      "201501\n",
      "201502\n",
      "201503\n",
      "201504\n",
      "201505\n",
      "201506\n",
      "201507\n",
      "201508\n",
      "201509\n",
      "201510\n",
      "201511\n",
      "201512\n",
      "201601\n",
      "201602\n",
      "201603\n",
      "201604\n",
      "201605\n",
      "201606\n",
      "201607\n",
      "201608\n",
      "201609\n",
      "201610\n",
      "201611\n",
      "201612\n",
      "201701\n",
      "201702\n",
      "201703\n",
      "201704\n",
      "201705\n",
      "201706\n",
      "201707\n",
      "201708\n",
      "201709\n",
      "201710\n",
      "201711\n",
      "201712\n",
      "201801\n",
      "201802\n",
      "201803\n",
      "201804\n",
      "201805\n",
      "201806\n",
      "201807\n",
      "201808\n",
      "201809\n",
      "201810\n",
      "201811\n",
      "201812\n",
      "201901\n",
      "201902\n",
      "201903\n",
      "201904\n",
      "201905\n",
      "201906\n",
      "201907\n",
      "201908\n",
      "201909\n",
      "201910\n",
      "201911\n",
      "201912\n",
      "202001\n",
      "202002\n",
      "202003\n",
      "202004\n",
      "202005\n",
      "202006\n",
      "202007\n",
      "202008\n",
      "202010\n",
      "202011\n",
      "202012\n",
      "202101\n",
      "202102\n",
      "202103\n",
      "202104\n",
      "202105\n",
      "202106\n",
      "202107\n",
      "202009\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>u</th>\n",
       "      <th>v</th>\n",
       "      <th>wdir</th>\n",
       "      <th>wspd</th>\n",
       "      <th>year_month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32012</th>\n",
       "      <td>32.0</td>\n",
       "      <td>-125.0</td>\n",
       "      <td>1.120695</td>\n",
       "      <td>-3.830908</td>\n",
       "      <td>286.306335</td>\n",
       "      <td>3.991468</td>\n",
       "      <td>202009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32013</th>\n",
       "      <td>32.5</td>\n",
       "      <td>-125.0</td>\n",
       "      <td>1.506737</td>\n",
       "      <td>-3.690114</td>\n",
       "      <td>292.21106</td>\n",
       "      <td>3.985875</td>\n",
       "      <td>202009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        lat    lon         u         v        wdir      wspd year_month\n",
       "32012  32.0 -125.0  1.120695 -3.830908  286.306335  3.991468     202009\n",
       "32013  32.5 -125.0  1.506737 -3.690114   292.21106  3.985875     202009"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = read_clean_wind()\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``census geom``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>ZCTA10</th>\n",
       "      <th>GEOID10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37.465</td>\n",
       "      <td>-117.936</td>\n",
       "      <td>89010</td>\n",
       "      <td>0689010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37.465</td>\n",
       "      <td>-117.935</td>\n",
       "      <td>89010</td>\n",
       "      <td>0689010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      lat      lon ZCTA10  GEOID10\n",
       "0  37.465 -117.936  89010  0689010\n",
       "1  37.465 -117.935  89010  0689010"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip_poly = read_census_geom()\n",
    "zip_poly.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Find zip (zcta) code for wind data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>ZCTA10</th>\n",
       "      <th>u</th>\n",
       "      <th>v</th>\n",
       "      <th>wdir</th>\n",
       "      <th>wspd</th>\n",
       "      <th>year_month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37.465</td>\n",
       "      <td>-117.936</td>\n",
       "      <td>89010</td>\n",
       "      <td>0.504258</td>\n",
       "      <td>-0.719008</td>\n",
       "      <td>305.042938</td>\n",
       "      <td>0.878208</td>\n",
       "      <td>199101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35.396</td>\n",
       "      <td>-116.322</td>\n",
       "      <td>89019</td>\n",
       "      <td>-0.172753</td>\n",
       "      <td>-0.94694</td>\n",
       "      <td>259.661102</td>\n",
       "      <td>0.962568</td>\n",
       "      <td>199101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      lat      lon ZCTA10         u         v        wdir      wspd year_month\n",
       "0  37.465 -117.936  89010  0.504258 -0.719008  305.042938  0.878208     199101\n",
       "1  35.396 -116.322  89019 -0.172753  -0.94694  259.661102  0.962568     199101"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final = add_zcta_to_wind(df, zip_poly)\n",
    "df_final.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Export data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv(out_dir  + 'winds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['wdir'] = df_final.wdir.astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    599311.000000\n",
       "mean        177.887713\n",
       "std         133.092457\n",
       "min           0.001104\n",
       "25%          40.066475\n",
       "50%         174.607162\n",
       "75%         319.594696\n",
       "max         359.999634\n",
       "Name: wdir, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.wdir.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
